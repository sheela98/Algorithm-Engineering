\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}

\begin{document}

\title{Algorithm Engineering}
\subtitle{Exam Assignments}


\author{Sheela Orgler}
\institute{Friedrich Schiller Universität, Jena} 

\maketitle

% \begin{abstract}
%
% \end{abstract}

\section{Assignment}

\subsection{Describe how parallelism differs from concurrency}
Parallelism is a subset of concurrency. If a system is parallel it is also concurrent, but not the other way around. A system is considered parallel if two or more tasks are executed simultaneously.
\\
Concurrency only means support for two or more actions at the same time. Parallelism refers to execution of more than one action at the same time.

\subsection{What is fork-join parallelism?}
Fork-join parallelism refers to a concept where we have a master thread that divides into a team of threads. With this concept we only have parallel regions where threads are executed simultaneously. In between these parallel regions the execution is sequential following the master thread. 
\\
We can imagine execution as a line that is followed from starting to end point. With fork-join parallelism this line is our master thread. Following our master thread at some point the thread can divide into two or more threads. These threads are joined after some time to our master thread. In these sections where we have two or more threads execution will be parallel and after these sections sequential. 


\subsection {Chapter 1 – Computer Systems: A Programmer’s Perspective}

\paragraph{Discuss one thing - Caches}
As discussed in Chapter 1 of the book caches are helpful to deal with the processor-memory gap. 

The processor can read data from the register file, which is within the CPU almost 100 times faster than from memory. The gap between processor and memory is continuously growing. 

Cache memory is used to bridge the gap. It is used to temporarily store information that will likely be used in the future. There exist several levels of caches -  L1, L2, L3 - which go from smallest to largest. 

A cache reduces the acess time to data in memory. Frequently used data and instructions are kept in the cache. Access to data kept there is faster.

Modern desktops, servers and industrial CPUs have at least three independent caches: instruction cache, data cache and the TLB. The instruction cache is used to speed up executable instruction fetches. The data cache speeds up data fetch and store. It is organized into more cache levels:
\begin{itemize}
	\item L1 cache - Primary cache is fast but small and usually embedded in the processor chip (CPU)
	\item L2 cache - Secondary cache is larger and can be embedded in the CPU or on a seperate chip or coprocessor containing a high-speed alternative system bus connectin cache and CPU.
	\item L3 cache - specialized memory to improve L1 and L2. L3 is usually double the speed of DRAM
\end{itemize}
The small caches L1 are backed up by larger and slower caches L2, L3 to address the tradeoff between cache latency and hit rate. The faster cache is generally checked firt. If that cache misses, the next chache is checked and so on, before accessing main memory.

For example the ARM-base Apple M1 CPU has 8 cores: 4 high-performance and four high-efficiency cores. The four high-performance cores have a 192 KiB L1 cache for each of the cores and the four high-efficiency cores only have 128 KiB.

The Translation lookaside buffer is used to speed up the translation from virtual to physical addresses. It is part of the memory management unit and not directly related to CPU caches.

% https://en.wikipedia.org/wiki/CPU_cache#First_instruction_cache
% https://www.techtarget.com/searchstorage/definition/cache-memory

\subsection {Paper – There’s plenty of room at the Top: What will drive computer performance after Moore’s law?}

\paragraph{Explain figure “Performance gains after Moore’s law ends”}
The figure illustrates the “Top” and “Bottom” referring to computer performance gains. The “Bottom” refers to the miniaturization of computer components seen in the last decades. \\
Due to physical limits the opportunities for gains at the bottom will slowly come to an end. Nevertheless, there are still opportunities for growth at the “Top”. The “Top” shows the three aspects where growth can be expected: software, algorithms, hardware architecture. These aspects are divided into technology, opportunity and examples all looking into the growth opportunities in the specific area.
\\
Growth opportunities at the “Top”:
\begin{itemize}
	\item making software more efficient by performance engineering
	\item minimizing the time it takes to run and not the development time
	\item an increasing number of processor cores running parallel
	\item reengineer modularity to obtain performance gains
\end{itemize}


\section{Assignment}

\subsection{What causes false sharing?}
False sharing occurs, when threads on different processors modify variables on the same cache line. A cache line is the smallest unit of memory. Its length depends on the underlying architecture and is typically 64 and in more recent architectures 128 bytes long.
\\
False sharing happens when for example two threads on different processors modify variables on the same cache line. The first thread modifies one variable at the beginning of the cache line. The second thread tries to modify a variable at the end of the cache line. Due to the alteration of the first thread, the cache line is invalidated for the second thread and has to be reloaded.
\\
False sharing only happens when variables are changed and not when they are only read. It may lead to a significant performance decrease.


\subsection{How do mutual exclusion constructs prevent race conditions?}
Race conditions occur when two threads modify the same data. Mutual exclusions give one thread exclusive access to the data. After execution exclusive access is returned and the next thread can execute.

\subsection{Explain the differences betweent static and dynamic schedules in OpenMP.}
Static and dynamic schedules differ in how the work \( iterations of the for loop \)) is spread across the threads. Static means that it is decided at the beginning and dynamic means that it is decided at runtime. Each thread will work on a chunk of values and then take the next chunk that hasn't been worked on by any thread.
Static schedules perform better for balanced workloads and dynamic schedules for unbalanced workloads, in case the workload varies between different iterations of the for loop.
The chunk size can be specifies; for static schedules it is one per default.

\subsection{What can we do if we've found a solution while running a parallel for loop in OpenMP, but still have many iterations left?}
If we've found a solution within a parallel for loop, we can use continue. We don't continue calculating, but quickly iterate through the loop to increase performance.


\subsection{Explain in your own words how std::atomic::compare\_exchange\_weak work}
strd::atomic::compare\_exchange\_weak  succeeds only if the value of the variable to be updated equals the first provided argument.
For example if for final\_solution.compare\_exchange\_weak(previous, i) final\_solution equals to previous. Previous is updated with i, if it fails.


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\begin{thebibliography}{8}

%\bibitem{ref_book1}
%Arpaci-Dusseau, Remzi H and Arpaci-Dusseau, Andrea C.: Operating systems: Three easy pieces. Arpaci-Dusseau Books LLCr,
 %(2018)

\bibitem{ref_url1}
Armv8-A Reference Manual pdf, pages 30-70, pages 1708-1808 \\ \url{https://developer.arm.com/documentation/ddi0487/ga}.


\bibitem{ref_url2}
Armv8-A Address Translation pdf, \url{https://developer.arm.com/documentation/100940/0101/}.

\end{thebibliography}
\end{document}
